# This is a page allocator micro-benchmark implemented in systemtap. The
# performance of this is indirectly important for a number of workloads
# but bear in mind that the cost of allocation for many workloads is
# dominated by the cost of zeroing memory rather than anything the
# allocator itself is doing
#
# Copyright 2011 Mel Gorman <mgorman@suse.de>
%{
#include <linux/fs.h>
#include <linux/types.h>
#include <linux/proc_fs.h>
#include <linux/kernel.h>
#include <linux/vmalloc.h>

#define PARAM_GFPFLAGS GFP_KERNEL
#define PARAM_ORDER 0
#define PARAM_ITERATIONS_INNER 100
#define PARAM_ITERATIONS_OUTER 10
#define PARAM_BATCH 1

#ifdef CONFIG_X86
/**
 * rdtsc: Read the current number of clock cycles that have passed
 */
inline unsigned long long read_clockcycles(void)
{
	unsigned long low_time, high_time;
	asm volatile( 
		"rdtsc \n\t" 
			: "=a" (low_time),
			  "=d" (high_time));
        return ((unsigned long long)high_time << 32) | (low_time);
}
#else
inline unsigned long long read_clockcycles(void)
{
	return jiffies;
}
#endif /* CONFIG_X86 */
%}

function alloc_runtest() %{
	unsigned int order = PARAM_ORDER;
	unsigned long batch = PARAM_BATCH;
	struct page **pages;		/* Pages that were allocated */
	int i, j;
	unsigned long long start_cycles_alloc, cycles_alloc;
	unsigned long long start_cycles_free, cycles_free;

	/* Allocate memory to store pointers to pages */
	pages = __vmalloc((PARAM_BATCH+1) * sizeof(struct page **),
			GFP_KERNEL|__GFP_HIGHMEM,
			PAGE_KERNEL);
	if (pages == NULL) {
		_stp_printf("Failed to allocate space to store page pointers\n");
		return;
	}
	memset(pages, 0, (PARAM_BATCH+1) * sizeof(struct page **));

	cycles_alloc = cycles_free = 0;

	for (i = 0; i < PARAM_ITERATIONS_OUTER; i++) {
		for (j = 0; j < PARAM_ITERATIONS_INNER; j++) {
			int nr_pages;

			/* No point hogging the CPU */
			cond_resched();

			/* Time allocations */
			start_cycles_alloc = read_clockcycles();
			for (nr_pages = 0; nr_pages <= batch; nr_pages++)
				pages[nr_pages] = alloc_pages(PARAM_GFPFLAGS | __GFP_NOWARN, order);
			cycles_alloc += read_clockcycles() - start_cycles_alloc;

			/* Time frees */
			start_cycles_free = read_clockcycles();
			for (nr_pages = 0; nr_pages <= batch; nr_pages++)
				if (pages[nr_pages] != NULL)
					__free_pages(pages[nr_pages], order);
			cycles_free += read_clockcycles() - start_cycles_free;
		}

		cycles_alloc /= PARAM_ITERATIONS_INNER;
		cycles_free /= PARAM_ITERATIONS_INNER;
		cycles_alloc /= batch;
		cycles_free /= batch;

		_stp_printf("order %2d batch %6d alloc %llu free %llu\n",
			order, batch, cycles_alloc, cycles_free);
	}

	vfree(pages);

	return;
%}

probe begin
{
	alloc_runtest()
	exit()
}
